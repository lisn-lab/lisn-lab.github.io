<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LISN Lab</title>
    <link>https://lisn-lab.org/</link>
      <atom:link href="https://lisn-lab.org/index.xml" rel="self" type="application/rss+xml" />
    <description>LISN Lab</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-gb</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://lisn-lab.org/media/icon_hu_efe3ed8cd119c4a9.png</url>
      <title>LISN Lab</title>
      <link>https://lisn-lab.org/</link>
    </image>
    
    <item>
      <title>Language</title>
      <link>https://lisn-lab.org/research/language/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/research/language/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#abstract-concepts&#34;&gt;Abstract Concepts&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#structural-processing&#34;&gt;Structural Processing&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#eye-movements-in-reading&#34;&gt;Eye Movements in Reading&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#language-and-emotion&#34;&gt;Language and Emotion&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;p&gt;At LISN, we investigate language processing through multiple complementary lenses, from fundamental cognitive mechanisms to complex emotional interactions. Our research spans four key areas: the representation of abstract concepts, structural processing within and across languages, eye movements during reading, and the interaction between language and emotion. Through this multi-faceted approach, we aim to develop a comprehensive understanding of how humans process, comprehend, and experience language. Our work combines diverse methodologies - from precise eye-tracking measurements to cognitive-behavioral studies - enabling us to examine language processing at various levels of analysis.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract-concepts&#34;&gt;Abstract Concepts&lt;/h2&gt;
&lt;p&gt;Abstract concepts like &amp;ldquo;love&amp;rdquo; or &amp;ldquo;trust&amp;rdquo; pose a unique challenge in cognitive science. Unlike concrete concepts such as &amp;ldquo;red,&amp;rdquo; which derive meaning from direct sensory experiences, abstract concepts lack physical referents. This absence raises questions about how such concepts are represented in the mind and complicates the prevailing &amp;rsquo;embodied&amp;rsquo; theory of concept representation, which suggests that concepts gain meaning through bodily experiences.&lt;/p&gt;
&lt;p&gt;At LISN, we tackle these challenges by exploring alternative grounding for abstract concepts. We investigate whether emotional or metaphorical experiences provide the basis for understanding abstract words. For example, &amp;rsquo;trust&amp;rsquo; is often considered a &amp;lsquo;big&amp;rsquo; concept because it elicits strong emotions &lt;a href=&#34;https://lisn-lab.org/publication/2013-YVWSOS-PlosONE&#34;&gt;(Yao et al., 2013)&lt;/a&gt; and is metaphorically associated with large objects like &amp;lsquo;castle&amp;rsquo; or &amp;lsquo;cathedral&amp;rsquo; &lt;a href=&#34;https://lisn-lab.org/publication/2022-YTS-JML&#34;&gt;(Yao, Taylor, &amp;amp; Sereno, 2022)&lt;/a&gt;. Our research also examines the possibility that abstract concepts have a more episodic and context-dependent grounding compared to concrete ones. For instance, the concept of &amp;rsquo;love&amp;rsquo; can vary significantly depending on the context, such as a romantic dinner or a care home, unlike more concrete concepts like &amp;lsquo;cat,&amp;rsquo; which consistently evoke features like fur and paws. We extend this inquiry to understand how embodied experiences influence language comprehension across various contexts and life stages.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;structural-processing&#34;&gt;Structural Processing&lt;/h2&gt;
&lt;p&gt;Structural processing in language is a complex cognitive function that extends beyond mere syntax. It involves breaking down sentences into components such as subjects, verbs, and objects, and understanding their interrelationships to extract meaning. This function also intersects with other cognitive domains like prosody - the intonation in spoken sentences - and arithmetic operations, highlighting its role as a domain-general mechanism crucial for structuring language, cognition, and communication.&lt;/p&gt;
&lt;p&gt;At LISN, our focus is twofold. First, we explore the link between implicit prosody in reading, commonly known as &amp;ldquo;inner speech,&amp;rdquo; and syntactic processing. This helps us understand how the &amp;lsquo;melody&amp;rsquo; of language in our minds influences sentence interpretation &lt;a href=&#34;https://lisn-lab.org/publication/2018-YS-Cognition&#34;&gt;(Yao &amp;amp; Scheepers, 2018)&lt;/a&gt;. Second, we study how sentence structures in different languages affect cognition among their speakers. For example, Chinese sentences often omit subjects and focus on topics, whereas English sentences require subjects and emphasise them. This difference could lead to varying attentional focuses and mental representations. We also examine the contrasting structures between left-branching languages like Chinese and right-branching languages like English, suggesting different structural hierarchies between languages. Through these investigations, we aim to untangle the complexities of structural processing and its influence on cognition and communication.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;eye-movements-in-reading&#34;&gt;Eye Movements in Reading&lt;/h2&gt;
&lt;p&gt;Eye movements in reading offer a rich data source for understanding cognitive processes such as attention, memory, and language comprehension. Captured through eye-tracking technology, these movements include quick jumps called saccades and brief pauses known as fixations, where most information absorption takes place. Factors like text complexity and reader familiarity affect the duration of these fixations and the length of saccades. The parafoveal region of the eye also provides a &amp;lsquo;preview&amp;rsquo; of upcoming words during saccades, facilitating smoother reading. Lexical variables such as word frequency, predictability, and orthography further influence these eye movements, offering insights into ocular control and cognitive processes in reading.&lt;/p&gt;
&lt;p&gt;At LISN, we engage in targeted research projects to explore this complex landscape. For instance, one study examines the interaction between word frequency and contextual predictability in relation to parafoveal preview, aiming to understand their combined impact on fixation durations &lt;a href=&#34;https://lisn-lab.org/publication/2018-SWLYH-QJEP&#34;&gt;(Sereno et al., 2018)&lt;/a&gt;. Another research line investigates how altering the perceptual quality of the parafoveal preview might affect the processing of the previewed word&amp;rsquo;s spelling, subsequently influencing lexical selection in later fixations. Through these studies, LISN aims to clarify the multifaceted factors that influence eye movements in reading, thereby enriching broader theories of cognition and language processing.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;language-and-emotion&#34;&gt;Language and Emotion&lt;/h2&gt;
&lt;p&gt;The processing of emotion words involves a complex interplay between language and emotional cognition. One layer of complexity arises from the interaction between a word&amp;rsquo;s concreteness and its emotional valence. For example, concrete emotion words like &amp;ldquo;kiss&amp;rdquo; or &amp;ldquo;snake&amp;rdquo; often elicit stronger emotional and cognitive responses than abstract ones like &amp;ldquo;love&amp;rdquo; or &amp;ldquo;fear,&amp;rdquo; likely because they more easily evoke sensory experiences. Beyond this, the processing of emotion words is influenced by various factors such as the individual&amp;rsquo;s current emotional state, cultural background, and the context in which the word appears. For instance, a positive mood may facilitate the processing of positively-valenced words, while a negative mood could have the opposite effect.&lt;/p&gt;
&lt;p&gt;At LISN, we focus on these nuanced interactions. We examine how concreteness and emotional valence in word processing interact, particularly in the context of alexithymia—a condition characterised by difficulties in identifying and describing emotions &lt;a href=&#34;https://lisn-lab.org/publication/2018-YKBSOS-JEPLMC&#34;&gt;(Yao et al., 2018)&lt;/a&gt;. We also investigate how emotional experiences provide an embodied basis for understanding abstract concepts, thereby extending the &amp;rsquo;embodied cognition&amp;rsquo; framework to include emotional and abstract language. For example, the abstract concept of &amp;ldquo;freedom&amp;rdquo; may be more deeply understood through the emotional experience of relief or exhilaration &lt;a href=&#34;https://lisn-lab.org/publication/2013-YVWSOS-PlosONE&#34;&gt;(Yao et al., 2013)&lt;/a&gt;. Additionally, we explore the role of mood states in emotion word processing, investigating how they can alter the perceived emotional charge of words &lt;a href=&#34;https://lisn-lab.org/publication/2015-SSYTO-FrontierPsych&#34;&gt;(Sereno et al., 2015)&lt;/a&gt;. Through these research avenues, LISN aims to offer a comprehensive understanding of emotion word processing, enriching both theoretical frameworks and practical applications in cognitive science.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inner Speech</title>
      <link>https://lisn-lab.org/research/innerspeech/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/research/innerspeech/</guid>
      <description>&lt;p&gt;Inner speech represents a fundamental aspect of human cognition and consciousness, manifesting in various forms and serving multiple functions in mental life. At LISN, we investigate this phenomenon through multiple complementary approaches: examining its varieties and manifestations, studying its role in reading comprehension, mapping its neural underpinnings, and exploring its contribution to consciousness and self-awareness. This comprehensive approach allows us to build a deeper understanding of how inner speech shapes human thought and experience.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;varieties-of-inner-speech&#34;&gt;Varieties of Inner Speech&lt;/h2&gt;
&lt;p&gt;Inner speech encompasses distinct varieties, each serving unique cognitive functions. Dialogic inner speech enables internal conversations where multiple perspectives are mentally represented, while condensed inner speech operates in abbreviated thought patterns without full syntactic structure. Evaluative inner speech facilitates self-reflection and assessment, and motivational inner speech serves to regulate behavior and emotional states. These varieties differ not only in their form but also in their cognitive and behavioral impacts. Understanding these distinctions is crucial for comprehending how inner speech supports various mental processes, from problem-solving to emotional regulation.&lt;/p&gt;
&lt;p&gt;At LISN, we employ multiple complementary methods to investigate these inner speech varieties. Through experience sampling, we capture real-time manifestations of different inner speech types across various cognitive tasks. Our innovative combination of think-aloud protocols with advanced EEG analysis helps identify distinct neural signatures for different inner speech subtypes. By applying machine learning techniques to these neural patterns, we&amp;rsquo;re developing new frameworks for understanding how different forms of inner speech contribute to cognitive processing. Through these research avenues, LISN aims to establish a comprehensive taxonomy of inner speech varieties and their roles in human cognition.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;hearing-words-off-the-page&#34;&gt;Hearing Words off the Page&lt;/h2&gt;
&lt;p&gt;When reading direct speech, many people report experiencing vivid inner voices, complete with distinctive tones, accents, and emotional qualities. This phenomenon represents a specialized manifestation of inner speech that bridges written language and auditory experience. The vividness of these inner voices can vary based on factors such as emotional engagement with the text, reading proficiency, and individual differences in cognitive style. This experience often enhances text comprehension and memory retention, particularly for dialogue-rich narratives.&lt;/p&gt;
&lt;p&gt;At LISN, our research has revealed the sophisticated neural and cognitive mechanisms underlying this phenomenon. Our groundbreaking studies have demonstrated increased neural activity in the auditory cortex during silent reading of direct speech &lt;a href=&#34;https://lisn-lab.org/publication/2011-YBS-JoCN&#34;&gt;(Yao, Belin, &amp;amp; Scheepers, 2011)&lt;/a&gt;, accompanied by synchronous auditory cortical oscillations &lt;a href=&#34;https://lisn-lab.org/publication/2021-YTBK-NeuroImage&#34;&gt;(Yao et al., 2021)&lt;/a&gt;. We&amp;rsquo;ve also found that eye movements during reading align with the natural rhythm of the imagined inner voice &lt;a href=&#34;https://lisn-lab.org/publication/2011-YS-Cognition&#34;&gt;(Yao &amp;amp; Scheepers, 2011)&lt;/a&gt;. Our current research explores how early experiences of learning to read aloud might establish automatic links between text and auditory experiences, and how inner voice simulation might serve as a prosocial mechanism reinforced by the brain&amp;rsquo;s reward system &lt;a href=&#34;https://lisn-lab.org/publication/2020-AMBMYF-JoCN&#34;&gt;(Alderson-Day et al., 2020)&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;neural-mechanisms-of-inner-speech&#34;&gt;Neural Mechanisms of Inner Speech&lt;/h2&gt;
&lt;p&gt;The neural architecture supporting inner speech involves a sophisticated network of brain regions working in concert. This network encompasses multiple circuits, including speech production mechanisms (involving motor planning, coordination, and execution areas) and speech perception mechanisms (involving various temporal and parietal regions that support phonological and semantic processing). These core circuits interact dynamically with areas responsible for memory, attention, and emotional processing, creating a complex neural symphony that enables various forms of inner speech.&lt;/p&gt;
&lt;p&gt;At LISN, our research focuses on understanding these neural mechanisms and how they support inner speech&amp;rsquo;s diverse phenomenology. We&amp;rsquo;ve highlighted the rhythmic nature of inner speech &lt;a href=&#34;https://lisn-lab.org/publication/2025-Y-PhysLife&#34;&gt;(Yao, 2025)&lt;/a&gt; by observing increased neural activity in auditory regions associated with speech prosody &lt;a href=&#34;https://lisn-lab.org/publication/2011-YBS-JoCN&#34;&gt;(Yao, Belin, &amp;amp; Scheepers, 2011)&lt;/a&gt; and more synchronous auditory cortical oscillations &lt;a href=&#34;https://lisn-lab.org/publication/2021-YTBK-NeuroImage&#34;&gt;(Yao et al., 2021)&lt;/a&gt;. Our recent meta-analysis &lt;a href=&#34;https://lisn-lab.org/publication/2023-PPY-NeuroImage&#34;&gt;(Pratts, Pobric, &amp;amp; Yao, 2023)&lt;/a&gt; suggests that inner speech may be supported by both a motor-driven pathway involving speech production circuits and an auditory pathway engaging perceptual mechanisms, potentially underpinning different inner speech subtypes. We are also employing machine learning to identify the neural features that characterise these subtypes, aiming for a more nuanced understanding of inner speech and its role in cognition. Through these research avenues, LISN seeks to deepen our understanding of the neural mechanisms of inner speech and its various subtypes.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;inner-speech-in-self-consciousness&#34;&gt;Inner Speech in Self-Consciousness&lt;/h2&gt;
&lt;p&gt;Inner speech plays a fundamental role in human consciousness, serving as a critical bridge between thought and self-awareness. This internal dialogue shapes our sense of self, influences decision-making, and mediates between different levels of consciousness, from focused attention to mind-wandering states. The relationship between inner speech and consciousness has profound implications for understanding both normal cognitive function and various psychological conditions, where alterations in inner speech often correspond to changes in self-awareness and conscious experience.&lt;/p&gt;
&lt;p&gt;At LISN, we&amp;rsquo;re pioneering new approaches to understanding this relationship through innovative research designs. Our studies capitalise on recent discoveries about aphantasia and other individual differences in mental experience to examine how variations in inner speech abilities affect self-awareness and consciousness. Using advanced EEG techniques, we&amp;rsquo;re developing objective measures of inner speech during self-referential processing, while our machine learning algorithms are creating new frameworks for classifying brain states related to different forms of inner dialogue. This research not only advances our theoretical understanding but also has practical implications for addressing conditions like rumination, depression, and auditory verbal hallucinations, where the relationship between inner speech and consciousness becomes disturbed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Voice Hearing</title>
      <link>https://lisn-lab.org/research/voicehearing/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/research/voicehearing/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#voice-hearing-continuum&#34;&gt;Voice Hearing Continuum&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#voice-hearing-as-predictions&#34;&gt;Voice Hearing as Predictions&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#voice-hearing-transition-risks&#34;&gt;Voice Hearing Transition Risks&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#voice-hearing-interventions&#34;&gt;Voice Hearing Interventions&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;p&gt;Voice hearing research spans multiple interconnected domains, from understanding its prevalence in the general population to developing innovative interventions. At LISN, we take a comprehensive approach to studying this complex phenomenon. Our research examines voice hearing across four key areas: its existence as a continuous spectrum in the population, the critical factors influencing transition to clinical conditions, the underlying predictive mechanisms in the brain, and the development of novel therapeutic interventions. By investigating these interrelated aspects, we aim to advance both theoretical understanding and practical treatments for voice hearing experiences.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;voice-hearing-continuum&#34;&gt;Voice Hearing Continuum&lt;/h2&gt;
&lt;p&gt;Voice hearing exists on a spectrum and is not limited to clinical populations. It involves a range of experiences, from hearing one&amp;rsquo;s name to complex dialogues, and can be distressing or benign. This natural variation in human experience has been studied through neuroscientific and psychological approaches, exploring its neural and cognitive correlates, while cultural factors also play a role. Understanding this spectrum has implications for mental health treatment, challenging associated stigma and informing therapeutic interventions.&lt;/p&gt;
&lt;p&gt;At LISN, our research focuses on cognitive abilities and biases predictive of voice hearing tendencies across this spectrum. We particularly examine signal detection theory (SDT) performance, where both nonclinical and clinical voice hearers often exhibit an externalising bias, perceiving internal voices as originating externally. Through rigorous experimental methods, we compare SDT performance among individuals to understand how this bias may be influenced by auditory verbal hallucination (AVH) proneness, variations in corollary discharge, memory inhibition, and other individual factors. Through this focus, LISN aims to offer a nuanced understanding of the cognitive mechanisms underlying the voice hearing spectrum.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;voice-hearing-as-predictions&#34;&gt;Voice Hearing as Predictions&lt;/h2&gt;
&lt;p&gt;Voice hearing has been increasingly studied within the predictive coding framework, which describes perception as a process of hypothesis testing. In this framework, auditory verbal hallucinations (AVH) may arise from an imbalance between top-down predictions and bottom-up sensory input. This fundamental brain mechanism has been explored through various factors, such as neurotransmitter systems, cognitive biases, and socio-emotional factors. This perspective offers a generalised understanding of voice hearing and opens new avenues for prognosis, diagnosis and treatment.&lt;/p&gt;
&lt;p&gt;At LISN, we focus on experimentally manipulating predictions and sensory input to understand voice hearing better. Methods include conditioning, articulatory suppression, earworm induction, and exposure to different types of auditory noises. We assess the impacts of these manipulations on signal detection task (SDT) performance and self-reported voice hearing experiences &lt;a href=&#34;https://lisn-lab.org/publication/2023-MCPKY-FiN&#34;&gt;(Mak et al., 2023)&lt;/a&gt;. By demonstrating how voice hearing arises from natural brain processes, we aim to develop methods to characterise voice hearing tendencies across the population. This research helps us understand its prevalence and transition risks to psychosis, while contributing to broader efforts to normalize these experiences and reduce stigma, particularly in underserved communities.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;voice-hearing-transition-risks&#34;&gt;Voice Hearing Transition Risks&lt;/h2&gt;
&lt;p&gt;The transition from voice hearing to psychosis is a critical but relatively uncommon area of study in mental health research. While many people experience voice hearing without transitioning to clinical states like psychosis, understanding the factors that contribute to this transition is crucial for early intervention. Research has identified risk factors such as frequency and distress level of voice hearing, comorbid symptoms, and environmental stressors. Neurobiological markers and the concept of a &amp;ldquo;clinical high-risk state&amp;rdquo; have also been studied to identify those at increased risk.&lt;/p&gt;
&lt;p&gt;In collaboration with the Greater Manchester Mental Health NHS Foundation Trust, LISN conducts pioneering research to understand the transition risks among nonclinical voice hearers. Our research focuses on understanding these risks as neurodevelopmental vulnerabilities, which manifest through increasingly complex voice hearing experiences. We also aim to understand the underlying cognitive and neural mechanisms that contribute to this transition. Through this targeted research, LISN seeks to offer insights into the factors that may lead to the transition from voice hearing to psychosis, with the goal of informing early intervention and prevention strategies.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;voice-hearing-interventions&#34;&gt;Voice Hearing Interventions&lt;/h2&gt;
&lt;p&gt;Interventions for voice hearing have evolved to include a range of approaches, from pharmacological treatments to psychological therapies like CBT and mindfulness. Advanced techniques like neurostimulation and neurofeedback offer targeted, brain-based interventions. Neurostimulation methods like TMS and tDCS modulate neural activity, while neurofeedback teaches self-regulation through real-time brain monitoring. These methods are less reliant on medication and offer the promise of more personalised treatment.&lt;/p&gt;
&lt;p&gt;At LISN, our focus is on developing an EEG-tDCS closed-loop type of online intervention for voice hearing. This innovative system uses EEG to identify voice hearing states in real-time, while tDCS provides targeted intervention to prevent escalation into psychotic episodes. While this cutting-edge approach is still under development, it represents the ultimate goal of our lab in studying voice hearing. Through this pioneering technology, LISN aims to offer a more immediate and targeted intervention, potentially revolutionising the treatment landscape for voice hearing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>When inner voices become art</title>
      <link>https://lisn-lab.org/post/20250901-voicesofthemind-workshop1/</link>
      <pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/post/20250901-voicesofthemind-workshop1/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The most beautiful experience we can have is the mysterious. It is the source of all true art and science.&amp;rdquo; &lt;br&gt; &amp;ndash; Albert Einstein&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The little voice in my head is one such mystery.&lt;/p&gt;
&lt;p&gt;We all have different relationships with the voice in our head. Mine chatters as I read, write, and think. But here&amp;rsquo;s the fascinating part - your inner experience might be completely different from mine.&lt;/p&gt;
&lt;p&gt;While science explains this phenomenon as our brain&amp;rsquo;s silent language production, the reality is far more diverse and intriguing. Some people experience abstract, non-verbal thoughts, others engage in rich inner monologues, and some hear multiple voices in conversation. This remarkable variety of inner experiences has long puzzled psychologists and fascinated artists alike.&lt;/p&gt;
&lt;p&gt;This July, we bridged the gap between science and art by hosting a workshop that invited young people to externalise their thought processes through creative expression. Armed with iPads, paper, keyboards, and microphones, participants transformed their inner worlds into tangible art pieces. We complemented their creative exploration with scientific context, demonstrating EEG recordings of brain activity to show how our thoughts manifest physically.&lt;/p&gt;
&lt;p&gt;The results were extraordinary. Some participants visualised their thoughts as rainbow patterns of emotions, others depicted their inner voice as radio waves, and some translated their mental experience into rhythmic compositions. Each piece offered a unique window into how differently we all experience our inner worlds.&lt;/p&gt;

















&lt;div class=&#34;gallery-grid&#34;&gt;

  
  
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-vimw1&#34; href=&#34;https://lisn-lab.org/media/albums/vimw1/a1.png&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/vimw1/a1_hu_23bf1c7493b84b.webp&#34; loading=&#34;lazy&#34; alt=&#34;a1.png&#34; width=&#34;250&#34; height=&#34;188&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-vimw1&#34; href=&#34;https://lisn-lab.org/media/albums/vimw1/a2.png&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/vimw1/a2_hu_27191147d5ebcbaf.webp&#34; loading=&#34;lazy&#34; alt=&#34;a2.png&#34; width=&#34;250&#34; height=&#34;188&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-vimw1&#34; href=&#34;https://lisn-lab.org/media/albums/vimw1/a3.png&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/vimw1/a3_hu_31a8426a53fbca1b.webp&#34; loading=&#34;lazy&#34; alt=&#34;a3.png&#34; width=&#34;250&#34; height=&#34;188&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-vimw1&#34; href=&#34;https://lisn-lab.org/media/albums/vimw1/fightingthought.png&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/vimw1/fightingthought_hu_15565aebf0596bc4.webp&#34; loading=&#34;lazy&#34; alt=&#34;fightingthought.png&#34; width=&#34;250&#34; height=&#34;175&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-vimw1&#34; href=&#34;https://lisn-lab.org/media/albums/vimw1/IMG_3737.jpg&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/vimw1/IMG_3737_hu_11757b4bf6dbd05e.webp&#34; loading=&#34;lazy&#34; alt=&#34;IMG_3737.jpg&#34; width=&#34;188&#34; height=&#34;250&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-vimw1&#34; href=&#34;https://lisn-lab.org/media/albums/vimw1/thebox.png&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/vimw1/thebox_hu_b52713d79cf2bcdd.webp&#34; loading=&#34;lazy&#34; alt=&#34;thebox.png&#34; width=&#34;250&#34; height=&#34;174&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

&lt;p&gt;This workshop is part of a larger collaborative project with artists to develop an interactive instrument that allows people to &amp;lsquo;jam out&amp;rsquo; their thoughts and feelings in audiovisual patterns. The prototype will debut at our follow-up workshop on 16th September, and we&amp;rsquo;re excited to see how people will use it to express their own inner experiences.&lt;/p&gt;
&lt;p&gt;Why does this matter? Because understanding the diversity of human consciousness isn&amp;rsquo;t just an academic pursuit - it&amp;rsquo;s a gateway to greater empathy and self-awareness. When we realise that everyone&amp;rsquo;s inner experience is unique, we become more accepting of our own mental processes and more understanding of others&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;So, are you curious about how your inner voice compares to others? Would you like to learn more about language, gender, and design in human-computer interactions? Join us on &lt;strong&gt;16th September 2025 @4-6pm at the &lt;a href=&#34;https://gregson.co.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregson Centre&lt;/a&gt;&lt;/strong&gt;, to explore your own mental landscape through our new interactive art installation.&lt;/p&gt;
&lt;p&gt;The session will be joined by Dr Bo Yao (Neuroscientist), Charlie Brown (Artist), Steven Hodges (Artist), and Dr Alice Ashcroft (Computer Scientist).&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-exclamation-triangle  pr-1 fa-fw&#34;&gt;&lt;/i&gt;Please contact our lead artist Charlie Brown at &lt;a href=&#34;mailto:charlie_m_brown@icloud.com&#34;&gt;charlie_m_brown@icloud.com&lt;/a&gt; for more information and to sign up for this event.&lt;/p&gt;
&lt;br&gt;
More Info:












  












&lt;div class=&#34;card-simple view-card&#34;&gt;

  
    


&lt;div class=&#34;article-metadata&#34;&gt;

  
  

  

  

  

  
  
  
  

  
  

&lt;/div&gt;

  

  
  
  
    
    
    &lt;a href=&#34;https://lisn-lab.org/projects/voicesofthemind/&#34; &gt;
      &lt;div class=&#34;img-hover-zoom&#34;&gt;
        &lt;img src=&#34;https://lisn-lab.org/media/voicesofthemind_royalsociety_hu_ca2299c553883805.webp&#34; height=&#34;455&#34; width=&#34;808&#34;
            class=&#34;article-banner&#34; alt=&#34;Voices of the Mind&#34; loading=&#34;lazy&#34;&gt;
      &lt;/div&gt;
    &lt;/a&gt;
  

  &lt;div class=&#34;section-subheading article-title mb-1 mt-3&#34;&gt;
    &lt;a href=&#34;https://lisn-lab.org/projects/voicesofthemind/&#34; &gt;Voices of the Mind&lt;/a&gt;
  &lt;/div&gt;

  

  

&lt;/div&gt;



</description>
    </item>
    
    <item>
      <title>Parafoveal preview benefits magnified</title>
      <link>https://lisn-lab.org/publication/2025-yhms-cognition/</link>
      <pubDate>Thu, 24 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2025-yhms-cognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Calling for Artists to Create Artworks of Inner Speech for Young People</title>
      <link>https://lisn-lab.org/post/20250409-voicesofthemind-artist/</link>
      <pubDate>Wed, 09 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/post/20250409-voicesofthemind-artist/</guid>
      <description>&lt;h2 id=&#34;project-overview--vision&#34;&gt;Project Overview &amp;amp; Vision&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://lisn-lab.org/projects/voicesofthemind/&#34;&gt;‘Voices of the Mind’&lt;/a&gt; is a public engagement project that explores inner speech - the internal dialogue we have with our minds.&lt;/p&gt;
&lt;p&gt;We aim to empower young people aged 14-18 from diverse backgrounds with knowledge about their inner experiences, create interdisciplinary dialogue between science and art, reduce stigma around different thought patterns, and make neuroscience accessible to those traditionally underrepresented in STEM fields.&lt;/p&gt;
&lt;p&gt;We are seeking artists to create engaging works that interpret and visualise inner speech / thoughts, helping young people understand, relate to, and reflect upon their inner speech while sparking curiosity about neuroscience and AI. Your work will feature in public exhibitions, workshops, and online platforms.&lt;/p&gt;
&lt;h2 id=&#34;commission-details&#34;&gt;Commission Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Total Budget Available&lt;/strong&gt;: £4,500 (can be allocated to one artist or be divided among multiple artists)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Timeline&lt;/strong&gt;: Commissions to be completed by end of September 2025&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Theme&lt;/strong&gt;: Inner speech, inner thoughts, and the experience of hearing voices within our minds&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;u&gt;&lt;strong&gt;Focus&lt;/strong&gt;&lt;/u&gt;: We particularly welcome proposals that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make abstract thought processes tangible and relatable&lt;/li&gt;
&lt;li&gt;Represent the diversity of inner speech / thought experiences&lt;/li&gt;
&lt;li&gt;Create opportunities for reflection on how we think and talk to ourselves in the mind&lt;/li&gt;
&lt;li&gt;Explore the boundaries between typical inner speech and more intrusive thoughts&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;u&gt;&lt;strong&gt;Format&lt;/strong&gt;&lt;/u&gt;: We prioritise:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Digital&lt;/strong&gt; art that can be shared online and incorporated into our digital platforms&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Portable&lt;/strong&gt; physical installations that can travel between multiple events&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive&lt;/strong&gt; elements that engage audiences and encourage participation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;u&gt;&lt;strong&gt;Example Project Ideas&lt;/strong&gt;&lt;/u&gt;: We welcome innovative approaches beyond these suggestions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Animated Short Film&lt;/u&gt;: A visual journey through inner thoughts, perhaps showing how they flow, interact, or evolve&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Interactive VR Environment&lt;/u&gt;: An immersive virtual space where users experience different forms of inner speech as they navigate (could be a VR app or a 2D POV film illustrating spontaneous thought patterns)&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Audio Installation&lt;/u&gt;: A layered soundscape using multiple voices to represent the variety and complexity of inner speech (we can provide recordings of real self-talk as source material)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;submission-requirements&#34;&gt;Submission Requirements&lt;/h2&gt;
&lt;p&gt;Please provide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Artist statement explaining your motivation and interest in this project&lt;/li&gt;
&lt;li&gt;A concept description&lt;/li&gt;
&lt;li&gt;Visual/auditory references, sketches or examples of the proposed work&lt;/li&gt;
&lt;li&gt;Technical requirements, production plan, and timeline for completion&lt;/li&gt;
&lt;li&gt;Budget breakdown&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;selection-criteria&#34;&gt;Selection Criteria&lt;/h2&gt;
&lt;p&gt;Proposals will be evaluated on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Artistic quality and innovation&lt;/li&gt;
&lt;li&gt;Feasibility within budget and timeline&lt;/li&gt;
&lt;li&gt;Alignment with project aims and audience&lt;/li&gt;
&lt;li&gt;Intractability, portability and adaptability for various settings&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contact-and-submission&#34;&gt;Contact and Submission&lt;/h2&gt;
&lt;p&gt;Please submit your proposal to &lt;a href=&#34;mailto:b.yao1@lancaster.ac.uk&#34;&gt;b.yao1@lancaster.ac.uk&lt;/a&gt; by &lt;strong&gt;30th May 2025&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We particularly encourage applications from artists from diverse backgrounds and those with connections to Northwest England.&lt;/p&gt;
&lt;br&gt;
More Info:












  












&lt;div class=&#34;card-simple view-card&#34;&gt;

  
    


&lt;div class=&#34;article-metadata&#34;&gt;

  
  

  

  

  

  
  
  
  

  
  

&lt;/div&gt;

  

  
  
  
    
    
    &lt;a href=&#34;https://lisn-lab.org/projects/voicesofthemind/&#34; &gt;
      &lt;div class=&#34;img-hover-zoom&#34;&gt;
        &lt;img src=&#34;https://lisn-lab.org/media/voicesofthemind_royalsociety_hu_ca2299c553883805.webp&#34; height=&#34;455&#34; width=&#34;808&#34;
            class=&#34;article-banner&#34; alt=&#34;Voices of the Mind&#34; loading=&#34;lazy&#34;&gt;
      &lt;/div&gt;
    &lt;/a&gt;
  

  &lt;div class=&#34;section-subheading article-title mb-1 mt-3&#34;&gt;
    &lt;a href=&#34;https://lisn-lab.org/projects/voicesofthemind/&#34; &gt;Voices of the Mind&lt;/a&gt;
  &lt;/div&gt;

  

  

&lt;/div&gt;



</description>
    </item>
    
    <item>
      <title>LISN Lab Celebrates a Memorable EPS Lancaster Experience</title>
      <link>https://lisn-lab.org/post/20250407-epslancaster/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/post/20250407-epslancaster/</guid>
      <description>&lt;p&gt;What a vibrant few days we had at the Experimental Psychology Society conference in Lancaster! The LISN Lab team returned to the lab energised, inspired, and with notebooks full of new ideas after engaging with the psychological research community.&lt;/p&gt;
&lt;p&gt;The conference halls buzzed with excitement as our researchers shared their latest discoveries. We dived deep into the fascinating world of how languages, which place different emphases on the subject or the overall topic, are underpinned by different structural representations in the brain. Meanwhile, conversations flourished around our presentations on how our life experiences shape abstract thinking - it turns out those episodic memories don&amp;rsquo;t just help us remember the past, they fundamentally influence how we mentally construct concepts like &amp;ldquo;freedom&amp;rdquo; or &amp;ldquo;justice&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ve really enjoyed the discussions about our work on inner speech. When that little voice in your head speaks, how do the motor and auditory speech mechanisms work together, differently and dynamically? Our findings on how perceptual reactivation plays a key role in shaping inner speech experiences into different forms prompted many attendees to reflect on their own inner speech experiences.&lt;/p&gt;

















&lt;div class=&#34;gallery-grid&#34;&gt;

  
  
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-2025eps&#34; href=&#34;https://lisn-lab.org/media/albums/2025eps/guyutalk.jpeg&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/2025eps/guyutalk_hu_c9cc25426814203b.webp&#34; loading=&#34;lazy&#34; alt=&#34;guyutalk.jpeg&#34; width=&#34;250&#34; height=&#34;188&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-2025eps&#34; href=&#34;https://lisn-lab.org/media/albums/2025eps/jiaxuantalk.jpeg&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/2025eps/jiaxuantalk_hu_4a76a9246dd84a8f.webp&#34; loading=&#34;lazy&#34; alt=&#34;jiaxuantalk.jpeg&#34; width=&#34;250&#34; height=&#34;203&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-2025eps&#34; href=&#34;https://lisn-lab.org/media/albums/2025eps/poster_guyujiaxuan.jpeg&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/2025eps/poster_guyujiaxuan_hu_5022cb7283dfd511.webp&#34; loading=&#34;lazy&#34; alt=&#34;poster_guyujiaxuan.jpeg&#34; width=&#34;188&#34; height=&#34;250&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-2025eps&#34; href=&#34;https://lisn-lab.org/media/albums/2025eps/poster.jpeg&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/2025eps/poster_hu_fd370e65bf96f9c8.webp&#34; loading=&#34;lazy&#34; alt=&#34;poster.jpeg&#34; width=&#34;188&#34; height=&#34;250&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

&lt;p&gt;The entire event flowed seamlessly thanks to the tremendous efforts of Padraic, Alice, and all the local organisers who transformed academic presentations into a genuine community experience. Their thoughtful planning created spaces not just for formal knowledge exchange, but for the kind of spontaneous interactions that often spark the most innovative ideas.&lt;/p&gt;
&lt;p&gt;As we settle back into our research routines, we carry with us new connections, fresh insights, and renewed enthusiasm for our research questions. Stay tuned as we build on these experiences in our upcoming work!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Voices of the Mind</title>
      <link>https://lisn-lab.org/projects/voicesofthemind/</link>
      <pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/projects/voicesofthemind/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Behavioural and Online Testing</title>
      <link>https://lisn-lab.org/facilities/behaviour/</link>
      <pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/facilities/behaviour/</guid>
      <description>&lt;p&gt;We maintain multiple climate-controlled or soundproof testing laboratories designed for behavioural experiments. These spaces support our research on self-talk and the behavioural correlates of inner speech and cognitive processing.&lt;/p&gt;
&lt;p&gt;Our research capabilities extend beyond physical labs through institutional access to Pavlovia.org, enabling online experiments with diverse participant samples that complement our in-lab studies on inner language.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Eye Tracking</title>
      <link>https://lisn-lab.org/facilities/eyetracking/</link>
      <pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/facilities/eyetracking/</guid>
      <description>&lt;p&gt;Our Eyelink 1000 eye tracking system allows us to examine how inner speech and cognitive processes are reflected in eye movement patterns. By measuring gaze patterns, fixations, and pupil responses, we can gain insights into attentional processes and cognitive load during internal language generation and processing tasks.&lt;/p&gt;
&lt;p&gt;We also have a couple of more portable Tobii systems lying around that can be used in fieldwork, for instance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>High-density EEG with 3D digitisation</title>
      <link>https://lisn-lab.org/facilities/eeg/</link>
      <pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/facilities/eeg/</guid>
      <description>&lt;p&gt;Our lab primarily uses an EGI 128-channel EEG system for high-density electroencephalographic recordings. This setup provides exceptional temporal resolution for capturing neural activity patterns associated with inner speech and language processing.&lt;/p&gt;
&lt;p&gt;The system is enhanced by a Polhemus Fastrak system which maps precise electrode positions on each participant&amp;rsquo;s scalp, creating detailed 3D models that improve the accuracy of source analysis.&lt;/p&gt;

















&lt;div class=&#34;gallery-grid&#34;&gt;

  
  
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-eeglab&#34; href=&#34;https://lisn-lab.org/media/albums/eeglab/eeghead.jpeg&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/eeglab/eeghead_hu_8ee6dc617447587a.webp&#34; loading=&#34;lazy&#34; alt=&#34;eeghead.jpeg&#34; width=&#34;250&#34; height=&#34;250&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-eeglab&#34; href=&#34;https://lisn-lab.org/media/albums/eeglab/egieegnet.jpeg&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/eeglab/egieegnet_hu_543e18fc63ca47a2.webp&#34; loading=&#34;lazy&#34; alt=&#34;egieegnet.jpeg&#34; width=&#34;187&#34; height=&#34;250&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-eeglab&#34; href=&#34;https://lisn-lab.org/media/albums/eeglab/netstation.jpeg&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/eeglab/netstation_hu_f52793083b98109e.webp&#34; loading=&#34;lazy&#34; alt=&#34;netstation.jpeg&#34; width=&#34;187&#34; height=&#34;250&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-eeglab&#34; href=&#34;https://lisn-lab.org/media/albums/eeglab/polhemus.jpeg&#34; &gt;
      &lt;img src=&#34;https://lisn-lab.org/media/albums/eeglab/polhemus_hu_c9b3acc558cfb9cb.webp&#34; loading=&#34;lazy&#34; alt=&#34;polhemus.jpeg&#34; width=&#34;250&#34; height=&#34;176&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>High-End Computing</title>
      <link>https://lisn-lab.org/facilities/hec/</link>
      <pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/facilities/hec/</guid>
      <description>&lt;p&gt;Advanced data analysis of EEG and physiological recordings is supported by Lancaster University&amp;rsquo;s &lt;a href=&#34;https://lancaster-hec.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;High End Computing (HEC) Cluster&lt;/a&gt;. This powerful computing resource provides access to over 14,000 computing cores and advanced GPU capabilities, enabling sophisticated signal processing and analysis techniques essential for extracting meaningful patterns from complex neurophysiological data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inner speechless: Is it possible to have no inner speech?</title>
      <link>https://lisn-lab.org/post/20250131-anendophasia/</link>
      <pubDate>Fri, 31 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/post/20250131-anendophasia/</guid>
      <description>&lt;p&gt;Close your eyes for a moment and listen. What do you hear? For most of us, beyond any external sounds, there&amp;rsquo;s that familiar voice inside our head - our inner speech.  But have you ever wondered, what if that voice weren’t there at all? Is it possible to have no inner speech?&lt;/p&gt;
&lt;p&gt;Most of us can’t imagine going about our daily lives without ‘the voice inside our head’;&lt;/p&gt;
&lt;p&gt;This inner voice mediates many of the mechanisms that allow us to perceive the world around us, such as reading and memory. It also encourages us when we need it, keeps our focus when our minds drift and helps us problem-solve and reason. Sometimes, it can even be exploited by our doubts and anxieties to talk us down. The 20th-century, Russian psychologist Lev Vygotsky, in his theory of cognitive development, proposed that we are born social beings. Over time, the conversations we have with those in our surroundings eventually become &lt;em&gt;‘internalised’&lt;/em&gt;, and we discuss our behaviour with ourselves.&lt;/p&gt;
&lt;p&gt;Through conversations with my peers, I learned just how varied this cognitive tool can be.&lt;/p&gt;
&lt;hr&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/Wm33v9spP9Q?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;I found that some individuals described their inner speech as largely grammatical, resembling full, structured sentences. Others reported that their inner voices focus exclusively on the semantic meaning, rather than the grammatical features. This type of inner speech is akin to jotting down brief notes that capture only the essential points, also known as &lt;em&gt;condensed inner speech&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Most people said they use their inner voices to evaluate their own behavior and actions—to criticize or encourage themselves, with varying degrees of each, and one individual even said their mind is focused on this ‘24/7’. This &lt;em&gt;‘evaluative inner speech’&lt;/em&gt; has been shown to affect our mental states and moods, with some people noting that it helps them feel calmer, while others said it makes them feel more anxious.&lt;/p&gt;
&lt;p&gt;The complexity of inner speech becomes even more apparent when we consider multilingual individuals. When discussing this topic with a bilingual friend, she shared that, although Hebrew was her first language and the primary language spoken at home, her inner speech has consistently been in English for as long as she can remember. She explained, ‘Even though I didn’t learn it first, it’s the language I’ve grown up around, went to school speaking and it&amp;rsquo;s the language I’m most comfortable using.’&lt;/p&gt;
&lt;p&gt;The fascination doesn’t stop there. There’s also a difference in how people experience their inner voices: some say it feels like they’re speaking to themselves, while others feel more like they’re listening to somebody else. Some also report more vivid inner speech when reading, a context in which many describe inner speech that varies in tone, pitch or even accent.&lt;/p&gt;
&lt;p&gt;Evidently, there are a diverse range of uses and forms of inner voice experience. Varying in terms of our perspectives and environments and producing further differences in our moods and mental states.&lt;/p&gt;
&lt;p&gt;This raises an intriguing question: Is it possible for someone to have no inner speech at all?&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Anendophasia&#34; srcset=&#34;
               /post/20250131-anendophasia/anendophasia2_hu_69235bdee1a5b10a.webp 400w,
               /post/20250131-anendophasia/anendophasia2_hu_f0f0ffa295334bd5.webp 760w,
               /post/20250131-anendophasia/anendophasia2_hu_cee99471ba334f33.webp 1200w&#34;
               src=&#34;https://lisn-lab.org/post/20250131-anendophasia/anendophasia2_hu_69235bdee1a5b10a.webp&#34;
               width=&#34;600&#34;
               height=&#34;600&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This area is receiving growing attention, and the answer appears to be yes.&lt;/p&gt;
&lt;p&gt;For most of us, there is still a line between our inner version of speech, and what researchers call ‘unsymbolised’ concepts – abstract thoughts that aren’t expressed in words. While some report calling upon images instead of words when thinking, others describe experiencing only these unsymbolized thoughts.&lt;/p&gt;
&lt;p&gt;Even when reading, some people who lack inner speech describe comprehension of sentences in ‘concepts’, purely abstract and remote from the written text in front of them. They report understanding meaning directly, without consciously processing the words themselves.  Some even describe struggling to think of sentences without saying them verbally.&lt;/p&gt;
&lt;p&gt;The existence of individuals who lack inner speech has been documented not only with subjective reports but also with objective, behavioural tasks. Researchers coined the term ‘anendophasia’ to describe a lack of inner speech.&lt;/p&gt;
&lt;p&gt;In a recent study, Nedergaard &amp;amp; Lupyan (2024) examined the differences in cognition between those with inner speech and anendophasics. They gave both groups tasks that typically rely upon inner speech and compared their performance. Their findings were intriguing, recall for verbal working memory, which is closely related to inner speech, was higher for those with inner speech than those without.&lt;/p&gt;
&lt;p&gt;Given that many individuals consistently report not using, or even not having, inner speech, we can conclude with confidence that the phenomenon of anendophasia is, in fact, a real experience for some people. When results appear to be the same in populations with and without inner speech, this may be because anendophasics have developed alternative cognitive mechanisms to handle behavioral tests.&lt;/p&gt;
&lt;p&gt;This fascinating dimension of human inner experience creates further interesting questions about the possible consequences of anendophasia on cognition, attention, perception, and mental wellbeing. The implications are far-reaching – from how we approach teaching children who might process words differently, to how we conduct talking therapy when some individuals might not experience thoughts in verbal form.&lt;/p&gt;
&lt;p&gt;What about you? Do you have inner speech? What is it like for you?  We&amp;rsquo;re particularly interested in hearing from people who don&amp;rsquo;t experience inner speech or who think differently without using words.&lt;/p&gt;
&lt;p&gt;In fact, we&amp;rsquo;re currently conducting an exciting study using EEG to understand how different people process memories, especially those who might not use inner speech. If you&amp;rsquo;d like to contribute to our understanding of this fascinating phenomenon, we&amp;rsquo;d love to hear from you and potentially have you participate in our research.&lt;/p&gt;


&lt;ul class=&#34;cta-group&#34;&gt;
  
  &lt;li&gt;
    &lt;a href=&#34;https://forms.office.com/Pages/ResponsePage.aspx?id=Ec2bnHqXnE6poLxzQJAWSkfutwuiw5VOt6BaX9Yki5pUNVI2QlFaNUlVVVpSU0ExMUhUN0hVODNWWS4u&#34;  class=&#34;btn btn-primary px-3 py-3&#34;&gt;I don&amp;rsquo;t have an inner speech! Sign Me Up!&lt;/a&gt;
  &lt;/li&gt;
  
  
&lt;/ul&gt;

</description>
    </item>
    
    <item>
      <title>Lack of mental imagery? You may be special!</title>
      <link>https://lisn-lab.org/post/20250121-aphantasia/</link>
      <pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/post/20250121-aphantasia/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#what-is-aphantasia&#34;&gt;What is Aphantasia?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#does-aphantasia-affect-everyday-life&#34;&gt;Does Aphantasia Affect Everyday Life?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-surprising-benefits-of-aphantasia&#34;&gt;The Surprising Benefits of Aphantasia&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#what-about-you&#34;&gt;What About You?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#contributing-to-research&#34;&gt;Contributing to Research&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;p&gt;Think of an apple in your mind. Can you see a red fruit with a glossy sheen, or… nothing at all? If it is the latter, then &lt;strong&gt;you may be part of 2-4% of the world’s population that are aphantasic!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t worry - aphantasia isn&amp;rsquo;t a disability or medical condition. It&amp;rsquo;s simply a different way that the mind processes information and thinks.&lt;/p&gt;
&lt;h2 id=&#34;what-is-aphantasia&#34;&gt;What is Aphantasia?&lt;/h2&gt;
&lt;p&gt;Aphantasia is when your brain doesn’t form or use mental images as part of your thinking or imagination. If your mind is like a TV, for someone with Aphantasia, it is as if the screen is blank, with no shapes or colours or… anything at all.&lt;/p&gt;
&lt;p&gt;This affects even the most familiar objects. For instance, someone I interviewed said they can’t even imagine objects they use every day, such as their phone. They can think about their phone as a concept but literally cannot picture it in their head!
Aphantasia can be congenital (from birth) or acquired (developed in later life, often due to illness / injury / mental health condition) and doesn’t need diagnosing. In fact, people often don’t even realise that they are aphantasic, even though they may notice from a young age they lack the ability to form mental images.&lt;/p&gt;
&lt;p&gt;For example, in a BBC news article Niel Kenmuir said that as a child, he couldn’t fall asleep counting sheep – not because he didn’t want to, but because he simply couldn’t see the sheep in his mind! He didn’t realise this was unusual until adulthood. Indeed, many people didn’t realise until later in life that they were aphantasic, but did notice from a young age that they could not picture things in their mind like others.&lt;/p&gt;
&lt;h2 id=&#34;does-aphantasia-affect-everyday-life&#34;&gt;Does Aphantasia Affect Everyday Life?&lt;/h2&gt;
&lt;p&gt;Aphantasia rarely affects everyday life, as it doesn’t stop them from doing things.
However, Aphantasia does impact certain abilities, such as not being able to picture what book characters look like when reading, or finding arts and geometry hard because they can’t manipulate images or shapes in their mind.&lt;/p&gt;
&lt;p&gt;Some Aphantasics experience very poor visual memory – it’d be a struggle for them to recall images from significant events like wedding or their first day at school, and sometimes even to recognise faces.&lt;/p&gt;
&lt;h2 id=&#34;the-surprising-benefits-of-aphantasia&#34;&gt;The Surprising Benefits of Aphantasia&lt;/h2&gt;
&lt;p&gt;While having Aphantasia isn’t a limitation – it’s just a different way of thinking. Research suggests it may actually enhance certain cognitive abilities. People with aphantasia often excel in analytical thinking and abstract reasoning, demonstrating superior factual memory and logical problem-solving skills. Additionally, some aphantasic individuals report practical advantages, such as being less affected by traumatic visual memories or disturbing descriptions in books or movies, as they process information conceptually rather than visually.&lt;/p&gt;
&lt;h2 id=&#34;what-about-you&#34;&gt;What About You?&lt;/h2&gt;
&lt;p&gt;Are you curious about how your own mind works? Can you picture an apple? Do you have mental imagery? Have you ever wondered if you might be aphantasic? Understanding your cognitive style can provide valuable insights into how you learn, process information, and interact with the world.&lt;/p&gt;
&lt;h2 id=&#34;contributing-to-research&#34;&gt;Contributing to Research&lt;/h2&gt;
&lt;p&gt;Your experience matters to the scientific community. As part of our ongoing research into neurodiversity, we&amp;rsquo;re conducting a study on how aphantasia affects cognition.&lt;/p&gt;
&lt;p&gt;If you think you are Aphantasic and would like to contribute to our understanding of this fascinating variation in human cognition, please join our research by signing up through the link below.&lt;/p&gt;


&lt;ul class=&#34;cta-group&#34;&gt;
  
  &lt;li&gt;
    &lt;a href=&#34;https://forms.office.com/Pages/ResponsePage.aspx?id=Ec2bnHqXnE6poLxzQJAWSrFgdYO91ptHjgdJqGlvLhxUNUQ4RDZONVdCNjlGVUJHSU1HVVZXT1E5OC4u&#34;  class=&#34;btn btn-primary px-3 py-3&#34;&gt;Yay! Sign Me Up!&lt;/a&gt;
  &lt;/li&gt;
  
  
&lt;/ul&gt;

</description>
    </item>
    
    <item>
      <title>It&#39;s about time: Rhythmic foundations of inner thought</title>
      <link>https://lisn-lab.org/publication/2025-y-physlife/</link>
      <pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2025-y-physlife/</guid>
      <description></description>
    </item>
    
    <item>
      <title>STARS Squad Kicks Off Journal Club with a Bang!</title>
      <link>https://lisn-lab.org/post/20241030-stars-journalclub/</link>
      <pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/post/20241030-stars-journalclub/</guid>
      <description>&lt;p&gt;Our new STARS squad launched their first journal club this week, and it was a hit! Maisie led an engaging discussion on the Variety of Inner Speech Questionnaire-Revised, sparking interesting conversations about inner speech, cultural norms, and individual experiences.&lt;/p&gt;
&lt;p&gt;We explored how questionnaires like the VISQ can be improved and validated, and potentially be adapted for clinical contexts. We also dipped our toes into the fascinating world of schizophrenia and predictive brain models.&lt;/p&gt;
&lt;p&gt;It wasn&amp;rsquo;t all work, though - Summer&amp;rsquo;s Halloween-themed biscuits added a festive touch to the proceedings. The enthusiasm in the room was palpable, with every member eager to contribute.&lt;/p&gt;
&lt;p&gt;This successful start sets the stage for an exciting term ahead. Each STAR will have the chance to shine, engaging with a diverse range of topics and lively debates. Stay tuned for more updates!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exciting Funding News!</title>
      <link>https://lisn-lab.org/post/20241001-apex-award/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/post/20241001-apex-award/</guid>
      <description>&lt;p&gt;We are thrilled to announce that our lab has been awarded &lt;a href=&#34;https://royalsociety.org/news/2024/09/apex-awards/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an APEX Award by the Royal Society&lt;/a&gt; for an exciting project towards EEG Classification of Auditory Verbal Hallucinations.&lt;/p&gt;
&lt;p&gt;Our research aims to advance the understanding and detection of internal speech experiences akin to auditory verbal hallucinations (AVHs) by combining electroencephalography (EEG) with explainable machine learning. This cross-disciplinary approach brings together experts from neuroscience, psychology, and computer science to develop a potentially game-changing method for monitoring and treating AVHs.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re excited about the possibilities this project opens up and the potential to improve the lives of those affected by AVHs. Stay tuned for updates as we embark on this exciting new chapter in our research!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transcranial electrical stimulation modulates emotional experience and metabolites in the prefrontal cortex in a donation task</title>
      <link>https://lisn-lab.org/publication/2024-mbmcabmhmmp-scirep/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2024-mbmcabmhmmp-scirep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion processing in concrete and abstract words: Evidence from eye fixations during reading</title>
      <link>https://lisn-lab.org/publication/2024-ysbms-cognemo/</link>
      <pubDate>Wed, 05 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2024-ysbms-cognemo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bridging phenomenology and neural mechanisms of inner speech: ALE meta-analysis on egocentricity and spontaneity in a dual-mechanistic framework</title>
      <link>https://lisn-lab.org/publication/2023-ppy-neuroimage/</link>
      <pubDate>Fri, 29 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2023-ppy-neuroimage/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Investigating the lateralisation of experimentally induced auditory verbal hallucinations</title>
      <link>https://lisn-lab.org/publication/2023-mcpky-fin/</link>
      <pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2023-mcpky-fin/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What can size tell us about abstract conceptual processing?</title>
      <link>https://lisn-lab.org/publication/2022-yts-jml/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2022-yts-jml/</guid>
      <description></description>
    </item>
    
    <item>
      <title>People</title>
      <link>https://lisn-lab.org/people/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reading direct speech quotes increases theta phase-locking: Evidence for cortical tracking of inner speech?</title>
      <link>https://lisn-lab.org/publication/2021-ytbk-neuroimage/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2021-ytbk-neuroimage/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mental simulations of phonological representations are causally linked to silent reading of direct versus indirect speech</title>
      <link>https://lisn-lab.org/publication/2021-y-jocognition/</link>
      <pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2021-y-jocognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Processing speech and thoughts during silent reading: Direct reference effects for speech by fictional characters in voice-selective auditory cortex and a Theory-of-Mind network</title>
      <link>https://lisn-lab.org/publication/2020-ambmyf-jocn/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2020-ambmyf-jocn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Glasgow Norms: Ratings of 5,500 words on nine scales</title>
      <link>https://lisn-lab.org/publication/2019-skbys-brm/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2019-skbys-brm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Direct speech quotations promote low relative-clause attachment in silent reading of English</title>
      <link>https://lisn-lab.org/publication/2018-ys-cognition/</link>
      <pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2018-ys-cognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>‘It’s hard to write a good article’: The online comprehension of excuses as indirect replies</title>
      <link>https://lisn-lab.org/publication/2018-swlyh-qjep/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2018-swlyh-qjep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Comprehension of indirect requests is influenced by their degree of imposition</title>
      <link>https://lisn-lab.org/publication/2018-slwyh-discourseprocesses/</link>
      <pubDate>Sat, 17 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2018-slwyh-discourseprocesses/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Differential emotional processing in concrete and abstract words</title>
      <link>https://lisn-lab.org/publication/2018-ykbsos-jeplmc/</link>
      <pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2018-ykbsos-jeplmc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Testing the limits of contextual constraint: Interactions with word frequency and parafoveal preview during fluent reading</title>
      <link>https://lisn-lab.org/publication/2018-shsyo-qjep/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2018-shsyo-qjep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Walking blindfolded unveils unique contributions of behavioural approach and inhibition to lateral spatial bias</title>
      <link>https://lisn-lab.org/publication/2016-wavy-cognition/</link>
      <pubDate>Mon, 01 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2016-wavy-cognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion word processing: Does mood make a difference?</title>
      <link>https://lisn-lab.org/publication/2015-ssyto-frontierpsych/</link>
      <pubDate>Mon, 24 Aug 2015 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2015-ssyto-frontierpsych/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inner voice experiences during processing of direct and indirect speech</title>
      <link>https://lisn-lab.org/publication/2015-ys-eaipisp/</link>
      <pubDate>Wed, 24 Jun 2015 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2015-ys-eaipisp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Familiarity with interest breeds gossip: Contributions of emotion, expectation, and reputation</title>
      <link>https://lisn-lab.org/publication/2014-ysmos-plosone/</link>
      <pubDate>Wed, 13 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2014-ysmos-plosone/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic size of abstract concepts: It gets emotional when you can’t see it</title>
      <link>https://lisn-lab.org/publication/2013-yvwsos-plosone/</link>
      <pubDate>Wed, 25 Sep 2013 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2013-yvwsos-plosone/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Brain ‘talks over’ boring quotes: Top-down activation of voice-selective areas while listening to monotonous direct speech quotations</title>
      <link>https://lisn-lab.org/publication/2012-ybs-neuroimage/</link>
      <pubDate>Sun, 15 Apr 2012 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2012-ybs-neuroimage/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contextual modulation of reading rate for direct versus indirect speech quotations</title>
      <link>https://lisn-lab.org/publication/2011-ys-cognition/</link>
      <pubDate>Thu, 01 Dec 2011 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2011-ys-cognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Silent reading of direct versus indirect speech activates voice-selective areas in the auditory cortex</title>
      <link>https://lisn-lab.org/publication/2011-ybs-jocn/</link>
      <pubDate>Sat, 01 Oct 2011 00:00:00 +0000</pubDate>
      <guid>https://lisn-lab.org/publication/2011-ybs-jocn/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
